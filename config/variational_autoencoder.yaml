max_epochs: 20
gpus: 1
batch_size: 128
layer_type: continuous2d
train_fraction: 1.0
rescale_output: False
periodicity: 2.0
nonlinearity: False
latent_dim: 128
gradient_clip_val: 0.0
segments: 2
n: 2
M_N: 0.0000256 #determines how much the KL factors into the loss.  Should be batch_size/data_size, but I've found it may need to be much smaller
optimizer:
  name: adam #adahessian #adam
  patience: 10
  factor: 0.1
  gamma: 0.9
  lr: 5.0e-3
encoder:
  n: ${n} # polynomial order
  channels: [3, 32, 64, 128]
  segments: ${segments}
  kernel_size: [3, 3, 3]
  stride: 2
  periodicity: ${periodicity}
  normalization: null # batch normalization option
  padding: 1
decoder:
  n: ${n}
  channels: [128, 64, 32, 16, 3]
  segments: ${segments}
  kernel_size: [3, 3, 3, 4]
  stride: 2
  periodicity: ${periodicity}
  normalization: null # batch normalization
  padding: 0

# This stuff is for optimization
defaults:
  - override hydra/sweeper: nevergrad
hydra:
  sweeper:
    optim:
      # name of the nevergrad optimizer to use
      # OnePlusOne is good at low budget, but may converge early
      optimizer: OnePlusOne
      # total number of function evaluations to perform
      budget: 100
      # number of parallel workers for performing function evaluations
      num_workers: 10
      # maximize: true  # comment out for maximization
    # default parametrization of the search space
    parametrization:
      n:
        init: 2
        lower: 2
        upper: 4
        integer: true
      segments:
        init: 2
        lower: 2
        upper: 10
        integer: true
      # periodicity:
      #   init: 2
      #   lower: 2
      #   upper: 8
      #   integer: true
      # latent_dim:
      #  init: 128
      #  lower: 32
      #  upper: 512
      #  integer: true
      # lr:
      #   init: 1e-5
      #   lower: 1e-7
      #   upper: 1e-3
      #   log: true
